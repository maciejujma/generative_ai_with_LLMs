{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset from Hugging Face "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The dataset consist +14k dialogues and theirs' summarisation.\n",
    "\"\"\"\n",
    "HUGGINGFACE_DATASET_NAME: str = \"knkarthick/dialogsum\"\n",
    "\n",
    "dataset = load_dataset(HUGGINGFACE_DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_example_dialogue(dialogue_index: str, dialogue_dataset: DatasetDict) -> None:\n",
    "    \"\"\"\n",
    "    Function printing the example dialogue from the dialogue dataset.\n",
    "    \"\"\"\n",
    "    dash_line: str = \"-\".join(\"\" for x in range(100))\n",
    "    print(\"Example \", dialogue_index)\n",
    "    print(dash_line)\n",
    "    print(\"INPUT DIALOGUE:\")\n",
    "    print(dialogue_dataset[\"test\"][dialogue_index][\"dialogue\"])\n",
    "    print(dash_line)\n",
    "    print(\"BASELINE HUMAN SUMMARY:\")\n",
    "    print(dataset[\"test\"][dialogue_index][\"summary\"], \"\\n\")\n",
    "    print(dash_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing an example of the dialogue and its summary\n",
    "EXAMPLE_INDICIES: List[int] = [40, 200]\n",
    "\n",
    "for idx in EXAMPLE_INDICIES:\n",
    "    print_example_dialogue(dialogue_index=idx, dialogue_dataset=dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "WARNING: Flan-t5-base is built using ~250B parameters and weights ~1GB \n",
    "(use SageMaker Studio Lab instead of running it locally)\n",
    "\"\"\"\n",
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the usage of the Tokenizer\n",
    "sentence: str = \"What time is it, Tom?\"\n",
    "\n",
    "sentence_encoded = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "sentence_decoded = tokenizer.decode(\n",
    "    sentence_encoded[\"input_ids\"][0], skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(\"ENCODED SENTENCE:\")\n",
    "print(sentence_encoded[\"input_ids\"][0])\n",
    "print(\"\\nDECODED SENTENCE:\")\n",
    "print(sentence_decoded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LLM without Prompt-Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_indices = [40, 200]\n",
    "dash_line: str = \"-\".join(\"\" for x in range(100))\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset[\"test\"][index][\"dialogue\"]\n",
    "    summary = dataset[\"test\"][index][\"summary\"]\n",
    "\n",
    "    inputs = tokenizer(dialogue, return_tensors=\"pt\")\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_new_tokens=50,\n",
    "        )[0],\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "\n",
    "    print(dash_line)\n",
    "    print(\"Example \", i + 1)\n",
    "    print(dash_line)\n",
    "    print(f\"INPUT PROMPT:\\n{dialogue}\")\n",
    "    print(dash_line)\n",
    "    print(f\"BASELINE HUMAN SUMMARY:\\n{summary}\")\n",
    "    print(dash_line)\n",
    "    print(f\"MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n{output}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero-shot inference i.e. we give instructions want we want to achieve without giving examples\n",
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset[\"test\"][index][\"dialogue\"]\n",
    "    summary = dataset[\"test\"][index][\"summary\"]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "\n",
    "    # Input constructed prompt instead of the dialogue.\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_new_tokens=50,\n",
    "        )[0],\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "\n",
    "    print(dash_line)\n",
    "    print(\"Example \", i + 1)\n",
    "    print(dash_line)\n",
    "    print(f\"INPUT PROMPT:\\n{prompt}\")\n",
    "    print(dash_line)\n",
    "    print(f\"BASELINE HUMAN SUMMARY:\\n{summary}\")\n",
    "    print(dash_line)\n",
    "    print(f\"MODEL GENERATION - ZERO SHOT:\\n{output}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-shot inference - we give one example of wanted input-output\n",
    "\n",
    "def make_prompt(example_indices_full, example_index_to_summarize):\n",
    "    prompt = ''\n",
    "    for index in example_indices_full:\n",
    "        dialogue = dataset['test'][index]['dialogue']\n",
    "        summary = dataset['test'][index]['summary']\n",
    "        \n",
    "        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred stop sequence.\n",
    "        prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "{summary}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
    "    \n",
    "    prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "\"\"\"\n",
    "        \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_indices_full = [40]\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=50,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ONE SHOT:\\n{output}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few-shot - we give a few examples (NOTE: if after 4-5 examples LLM is not giving desired output finetune model)\n",
    "example_indices_full = [40, 80, 120]\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=50,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
